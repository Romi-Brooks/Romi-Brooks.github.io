<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Arch Linux 下 Nvidia 显卡驱动的安装</title>
    <url>/2023/01/20/Arch%20Linux%20%E4%B8%8B%20Nvidia%20%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>注意，不是你卡了，是我还没写任何的内容 :(</p>
<span id="more"></span>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Archlinux</tag>
        <tag>Nvidia</tag>
        <tag>Installation</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx中code-server的ws配置</title>
    <url>/2023/04/01/Nginx%E4%B8%ADcode-server%E7%9A%84ws%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p><img src="/images/posts/Nginx%E4%B8%ADcode-server%E7%9A%84ws%E9%85%8D%E7%BD%AE/Cover.png" alt="封面" title="Cover"></p>
<blockquote>
<p>这个问题已经有很多人出现了,github也有人提出了issues<br>
<a href="https://github.com/coder/code-server/issues/4443">https://github.com/coder/code-server/issues/4443</a></p>
</blockquote>
<span id="more"></span>
<blockquote>
<p>可以看到导致这个问题的原因是由于<code>Nginx</code>反向代理并没有接受ws握手导致的<br>
这里也有很多人给出了答案,但是为了方便我之后部署,我还是决定把它写下来</p>
</blockquote>
<p>废话不多说,直接上<code>.conf</code>文件:</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line">server&#123;</span><br><span class="line">    <span class="comment">#其他选项</span></span><br><span class="line"></span><br><span class="line">    <span class="section">location</span> / &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span>  http://127.0.0.1:7654;  <span class="comment">#转发地址</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#下面这些就是相关的ws转发,把这些写上了code-server才能正常工作</span></span><br><span class="line">        <span class="attribute">proxy_set_header</span> HOST <span class="variable">$host</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Upgrade <span class="variable">$http_upgrade</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Connection upgrade;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Accept-Encoding gzip;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> Host <span class="variable">$proxy_host</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样,这也是一篇随笔…</p>
<p align="right">在最后的最后，感恩</p>
<p align="right">Romi Brooks♥</p>
]]></content>
      <categories>
        <category>WebDev</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>WebDev</tag>
        <tag>Website</tag>
        <tag>code-server</tag>
      </tags>
  </entry>
  <entry>
    <title>通过控制LFO Speed，使你的音乐更有律动</title>
    <url>/2022/04/27/FL-Studio-Tips-%E5%88%A9%E7%94%A8LFO-LFO-Speed%E5%81%9A%E5%87%BA%E5%8A%A0%E8%8A%B1%E6%95%88%E6%9E%9C/</url>
    <content><![CDATA[<p><img src="/images/posts/FL-Studio-Tips-LFO-Speed/cover.png" alt="封面" title="Cover"></p>
<audio id="audio" controls="" preload="none">
    <source id="mp3" src="/musics/posts/FL-Studio-Tips-LFO-Speed/FutureBass.mp3">
</audio>
<p><em>（音乐来自：FL Studio中DemoSong内Asher Postman-Future Bass）</em></p>
<p>在了解这之前，您可以点击上方的“播放按钮”，来听一首歌，这首歌用到了我所谓<code>控制 LFO Speed</code>的技巧 (在听之前请注意音量)</p>
<span id="more"></span>
<h1>什么是<code>LFO Speed</code>?</h1>
<p>LFO（Low Frequency Oscillator，低频振荡器）是一种产生周期性信号的调制源，其频率通常低于 20Hz（人类听觉下限），因此无法直接被听到，但可以控制其他参数的变化。例如：</p>
<p>调制音量：通过周期性增减振幅，产生类似 “脉冲” 的抽吸效果（即震音，Tremolo）。<br>
调制滤波器截止频率：通过周期性开合滤波器，使声音在高频和低频之间切换，形成 “摇晃” 的抽吸感（如 Dubstep 贝斯）。</p>
<p>而LFO Speed 决定了振荡的频率，直接影响抽吸效果的节奏。</p>
<h1>为什么要<code>控制 LFO Speed</code>?</h1>
<p>在音乐制作中，<strong>控制LFO Speed（低频振荡器速度）</strong> 是创造动态抽吸效果的核心技巧。<br>
这种效果通过周期性调制声音参数（如音量、滤波器截止频率），使音频产生类似 “呼吸” 或 “脉冲” 的节奏性波动，常见于电子音乐、Future Bass、Dubstep 等风格中。<br>
通过灵活运用 LFO Speed 和波形设计，制作人可以为声音注入生命般的动态变化。从微妙的音色润色到夸张的特效设计，是一个相对高级的声设技巧。</p>
<h1>它的效果具体是如何?</h1>
<p>这是上面那首Demo Song中使用此技巧的一小段 (在听之前请注意音量):</p>
<audio id="audio" controls="" preload="none">
    <source id="mp3" src="/musics/posts/FL-Studio-Tips-LFO-Speed/FutureBassLFO2.mp3">
</audio>
<p>这是去掉鼓点等干扰的效果:</p>
<audio id="audio" controls="" preload="none">
    <source id="mp3" src="/musics/posts/FL-Studio-Tips-LFO-Speed/FutureBassNDLFO2.mp3">
</audio>
<p>其实还有一次，是在这里:</p>
<audio id="audio" controls="" preload="none">
    <source id="mp3" src="/musics/posts/FL-Studio-Tips-LFO-Speed/FutureBassLFO1.mp3">
</audio>
<p>这是去掉鼓点等干扰的效果:</p>
<audio id="audio" controls="" preload="none">
    <source id="mp3" src="/musics/posts/FL-Studio-Tips-LFO-Speed/FutureBassNDLFO1.mp3">
</audio>
<h1>我是如何发现的?</h1>
<p>其实只要听的歌曲足够多，会发现这种效果用到的风格很多，哪怕是最基础的一个<code>crash下降</code>或是<code>Drop Melody</code>等很多效果，都能用这个技巧营造出很多听众的<code>爽点</code>。<br>
当然，具体操作的原理以及方式我是通过开头的那首Demo Song Project学到的：</p>
<p>来自 Asher Postman 的 Future Bass，我的fb启蒙制作者，包括其中的一种SideChain方式也是从这个工程学的。</p>
<blockquote>
<p>Title: Future Bass (Stock Plugins Only)<br>
Author: <a href="https://soundcloud.com/asher-postman">Asher Postman</a><br>
Copyright © 2018 Asher Postman.</p>
</blockquote>
<h2 id="如何实现">如何实现?</h2>
<p>来看两张图：<br>
<img src="/images/posts/FL-Studio-Tips-LFO-Speed/1.gif" alt="1" title="1"><br>
<img src="/images/posts/FL-Studio-Tips-LFO-Speed/2.gif" alt="2" title="2"></p>
<p>它们都有一个共性：<strong>其中的一个AMC（Automation Clips）均为&quot;LFO Speed&quot;，伴随着LFO Speed的降低，它们所控制的LFO频率也随之降低。</strong></p>
<p>上面说，“它们所控制的LFO频率”，那么我们首先得创作出一个LFO的AMC，这里FL Studio给大家了一种方法：</p>
<pre><code>  1.在您要实施此效果的对应旋钮上新建一个 AMC (Create automation clip)
  2.在 View Playlist 找到它，单击它名字旁边的AMC图标
  3.点击Channel Settings
    （1.一个很明显的 LFO 选项，打开它，之后设计您想要的波形（我们先不管speed，您可以设定其他几个值）
    （2.右键SPEED旋钮，新建一个AMC
    （3.现在您得到了两个AMC，这正是我们想要的结果，接下来就要靠您的听感了
  4.在您听感的基础上，调节LFO speed AMC使其满足您的要求
  5.完成
</code></pre>
<h1>实践&amp;展示</h1>
<h2 id="一-做futurebass的wobblechords-无lfo-speed-amc-当然我们也可以加-：">一，做FutureBass的WobbleChords（无LFO Speed AMC，当然我们也可以加）：</h2>
<p><img src="/images/posts/FL-Studio-Tips-LFO-Speed/1.png" alt="1" title="1"></p>
<h2 id="二-做hippop的melodysfx：">二，做Hippop的MelodySFX：</h2>
<p><img src="/images/posts/FL-Studio-Tips-LFO-Speed/2.png" alt="2" title="2"></p>
<blockquote>
<p>Tips：在这里，我们可以用Wave，Pulse等AMC Point Mode，右键AMC节点选择对应的Mode即可。</p>
</blockquote>
<h1>In the End</h1>
<p>为了显得最后的结束不算很唐突，我用了<a href="https://www.youtube.com/user/OfficialVirtualRiot">Virtual Riot</a> 的采样包，堆叠了一个Melodic Dubstep.<br>
just for fun，and thank u VR</p>
<p><img src="/images/posts/FL-Studio-Tips-LFO-Speed/3.png" alt="3" title="3"></br></p>
<audio id="audio" controls="" preload="none">
    <source id="mp3" src="/musics/posts/FL-Studio-Tips-LFO-Speed/Md.mp3">
</audio>
<p align="right">在最后的最后，感恩</p>
<p align="right">Romi Brooks♥</p>]]></content>
      <categories>
        <category>Music</category>
      </categories>
      <tags>
        <tag>FL-Tips</tag>
      </tags>
  </entry>
  <entry>
    <title>一份自用的ArchLinux安装指南</title>
    <url>/2023/01/09/Archlinux%20+%20U%E7%9B%98%20%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/</url>
    <content><![CDATA[<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/Cover.jpg" alt="封面" title="Cover"></p>
<h1>最后一次编辑时间: 2025-10-23 25:26</h1>
<ul>
<li>上一次编辑时间：2023-01-17 17:53</li>
<li>上一次编辑时间：2023-01-15 16:53</li>
<li>上一次编辑时间：2023-01-10 0:51</li>
</ul>
<div style="text-align: center;">
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1835463109&auto=0&height=66"></iframe>
</div>
<blockquote>
<p>参考ref<br>
1.一个完整的Arch Linux 安装流程: <a href="https://zhuanlan.zhihu.com/p/513859236">2022.5 archlinux详细安装过程</a><br>
2.Installation guide - ArchWiki: <a href="https://wiki.archlinuxcn.org/wiki/%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97">Arch Linux 安装指南</a></p>
</blockquote>
<p>本文是一个自用的Arch Linux安装方法，针对的是我自己，如果你看到这个文章了，里头可能有一些东西你是可以用到的。<br>
但是在阅读本指南前确保文档最后的更新时间和当前观看时间是否相近, Arch Linux是一个滚动更新系统，过时的安装过程也许会导致一些未知的问题出现。<br>
这篇文章旨在帮助不会安装Arch Linux的用户，以及当我自己遗忘某些安装细节时可以回过头来查看。<br>
可能有很多的安装方式与命令随着版本更新而无法使用或者不在推荐，出现问题或者是疑问应当自行查询。</p>
<span id="more"></span>
<h1>安装Arch Linux</h1>
<h2 id="引导-分区-调整-安装：">引导，分区，调整，安装：</h2>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.1.png" alt="1.1.1" title="1.1.1"></p>
<h3 id="确保使用uefi引导：">确保使用UEFI引导：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ls</span> /sys/firmware/efi/efivars</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.2.png" alt="1.1.2" title="1.1.2"></p>
<p>输入这一行代码后，如果反馈目录不存在，不要继续，因为没有UEFI启动在后面写入引导的时候几乎绝对报错。如果有输出，说明电脑是以UEFI启动的。</p>
<blockquote>
<p>Tips: 在这里我写入linux image用<code>Rufus</code>。</p>
</blockquote>
<h3 id="联网：">联网：</h3>
<p>1.输入“iwctl”进入iwd模式，也就是终端最前方有“[iwd]#”字样。<br>
2.在iwd模式下输入“device list”，查询电脑的网卡。记住你的网卡号，一般是wlan0或者wlan1（本文以多数情况wlan0为例）<br>
3.在iwd模式下输入“station wlan0 scan”，然后再输入“station wlan0 get-networks”，显示周围的wifi的ssid扫描结果。<br>
4.在iwd模式下输入“station wlan0 connect <ssid>”，如果是加密的wifi，系统会提示输入wifi密码，如无意外，就连上网了。<br>
5.验证联网。按ctrl+c退出iwd模式，回到[root@archiso~]模式，输入“ping <a href="http://baidu.com">baidu.com</a>”，如果有返回数据，说明已经连上网了。</p>
<blockquote>
<p>一般来说，如果是网线直连，那么系统会自动连接网络，而不用使用iwd连接wifi。</p>
</blockquote>
<h3 id="更新时间：">更新时间：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">timedatectl set-ntp <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3 id="分区：">分区：</h3>
<blockquote>
<p>涉及数据相关，请认真对待！</p>
</blockquote>
<h4 id="这里列出的是当前所有硬盘-找到你的硬盘-我这里是-dev-sda">这里列出的是当前所有硬盘，找到你的硬盘，我这里是/dev/sda</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lsblk</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.3.png" alt="1.1.3" title="1.1.3"></p>
<h4 id="进-dev-sda分盘">进/dev/sda分盘</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cfdisk /dev/sda</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.4.png" alt="1.1.4" title="1.1.4"></p>
<h4 id="gpt分区表-回车就行">gpt分区表，回车就行</h4>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.5.png" alt="1.1.5" title="1.1.5"></p>
<h4 id="用new新建分区-大小自定义-按照你自己来">用New新建分区，大小自定义，按照你自己来</h4>
<blockquote>
<p>sda1 -&gt; 1G boot分区  （引导分区，通常来说我们给1G即可）<br>
sda2 -&gt; 16G swap分区  （swap的大小通常为你电脑RAM的60%，根据你空余的硬盘空间设定适当的大小）<br>
sda3 -&gt; 60G /分区  (根目录大小会影响可安装软件的数量等，理解成Windows下的系统盘大小)<br>
sda4 -&gt; 50G /home分区  （此教程分开了home卷和根目录卷，分开维护使得数据不易丢失）</p>
</blockquote>
<h4 id="用write-然后输入yes写入分区">用Write，然后输入yes写入分区</h4>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.6.png" alt="1.1.6" title="1.1.6"></p>
<h4 id="用quit退出-然后lsblk看分的是否正确">用Quit退出，然后lsblk看分的是否正确</h4>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.7.png" alt="1.1.7" title="1.1.7"></p>
<h4 id="格式化">格式化</h4>
<p>这里需要注意的是:<br>
<code>/dev/sda1</code>是Linux的Boot分区，所以我们使用<code>FAT32</code>的文件系统来格式化次分区。<br>
<code>/dev/sda3</code>与<code>/dev/sda4</code>是Linux的根分区，根据不同需求推荐的文件系统类型也不同，<a href="https://zhuanlan.zhihu.com/p/1887926575850304663">这里有一个文章</a>，可以以此为基准。<br>
<code>/dev/sda2</code>是swap分区，使用如下命令开启swap。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkfs.fat -F32 /dev/sda1</span><br><span class="line">mkfs.ext4 /dev/sda3</span><br><span class="line">mkfs.xfs /dev/sda4</span><br><span class="line">mkswap /dev/sda2</span><br><span class="line">swapon /dev/sda2</span><br></pre></td></tr></table></figure>
<h4 id="挂载">挂载</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount /dev/sda3 /mnt</span><br><span class="line"><span class="built_in">mkdir</span> -p /mnt/boot/efi</span><br><span class="line">mount /dev/sda1 /mnt/boot/efi</span><br><span class="line"><span class="built_in">mkdir</span> /mnt/home</span><br><span class="line">mount /dev/sda4 /mnt/home</span><br></pre></td></tr></table></figure>
<blockquote>
<p>确保分区挂载正确，这里的参考应该是上文你所创建的分区！</p>
</blockquote>
<h4 id="检查">检查</h4>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.8.png" alt="1.1.8" title="1.1.8"></p>
<h3 id="配置pacman">配置pacman</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/pacman.d/mirrors</span><br></pre></td></tr></table></figure>
<blockquote>
<p>无论这个页面的内容是什么，只需要无脑修改第一条为bfsu的镜像源 (i luv u bfsu)</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Server = https://mirrors.bfsu.edu.cn/archlinux/<span class="variable">$repo</span>/os/<span class="variable">$arch</span>  </span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.9.png" alt="1.1.9" title="1.1.9"></p>
<h4 id="同步数据包">同步数据包</h4>
<blockquote>
<p>注意，不升级！</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -Sy</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.10.png" alt="1.1.10" title="1.1.10"></p>
<h4 id="安装archlinux-keyring防止签名出现问题">安装<code>archlinux-keyring</code>防止签名出现问题</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S archlinux-keyring</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.11.png" alt="1.1.11" title="1.1.11"></p>
<h3 id="安装系统到u盘">安装系统到U盘</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacstrap /mnt base linux linux-headers linux-firmware base-devel</span><br></pre></td></tr></table></figure>
<blockquote>
<p>ArchLinux有很多内核可以选择，这是一些ref:<br>
Stable 内核(linux)：标准 Linux 内核，附加少量补丁，适合大多数用户。<br>
Hardened 内核(linux-hardened)：专注于安全性，包含强化补丁以缓解漏洞。<br>
Longterm (LTS) 内核(linux-lts)：长期支持版本，适合需要稳定性的服务器或长期项目。<br>
Realtime 内核(linux-rt / linux-rt-lts)：提供实时性能，适用于需要低延迟的场景，如音频处理。<br>
Zen 内核(linux-zen)：优化日常使用体验，适合桌面用户。</p>
</blockquote>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.12.png" alt="1.1.12" title="1.1.12"></p>
<h4 id="写分区表">写分区表</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">genfstab -U /mnt &gt;&gt; /mnt/etc/fstab</span><br><span class="line"><span class="built_in">cat</span> /mnt/etc/fstab</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里<code>-U</code>是指使用uuid挂载硬盘，这会避免一些因为切换硬盘顺序而导致的无法加载系统的问题。</p>
</blockquote>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.13.png" alt="1.1.13" title="1.1.13"></p>
<h4 id="chroot">chroot</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">arch-chroot /mnt</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.14.png" alt="1.1.14" title="1.1.14"></p>
<h4 id="安装必要软件">安装必要软件</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S neovim iwd ttf-dejavu sudo bluez usbmuxd networkmanager dhcpcd wqy-zenhei ntfs-3g</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.15.png" alt="1.1.15" title="1.1.15"></p>
<h4 id="软连接neovim到vim">软连接neovim到vim</h4>
<blockquote>
<p>这何尝不是一种ntr(</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /bin/nvim /bin/vim</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.16.png" alt="1.1.16" title="1.1.16"></p>
<h4 id="同步时区">同步时区</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ln</span> -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">hwclock --systohc</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.17.png" alt="1.1.17" title="1.1.17"></p>
<h4 id="语言上的设置">语言上的设置</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/locale.gen</span><br></pre></td></tr></table></figure>
<p>这里取消掉<code>en_US.UTF-8</code> , <code>zh_CN.UTF-8</code> , <code>zh_TW.UTF-8</code> ，这几个地方的注释<br>
也就是前面的<code>#</code>号</p>
<p>然后生成语言文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">locale-gen</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.18.png" alt="1.1.18" title="1.1.18"></p>
<h4 id="追加-当前系统语言-到系统文件">追加 <code>当前系统语言</code> 到系统文件</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> LANG=en_US.UTF-8 &gt;&gt; /etc/locale.conf</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.19.png" alt="1.1.19" title="1.1.19"></p>
<h4 id="设定root密码">设定<code>root</code>密码</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">passwd</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.20.png" alt="1.1.20" title="1.1.20"></p>
<h4 id="主机名">主机名</h4>
<blockquote>
<p>设定成你自己喜欢的</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> livearch &gt;&gt; /etc/hostname</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.21.png" alt="1.1.21" title="1.1.21"></p>
<h4 id="本地回环">本地回环</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure>
<p>添加以下内容</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">127.0.0.1 localhost</span><br><span class="line">::1 localhost</span><br><span class="line">127.0.1.1 livearch.localdomain livearch  <span class="comment">#跟着上一步改</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.22.png" alt="1.1.22" title="1.1.22"></p>
<h4 id="安装引导">安装引导</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S grub efibootmgr</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.23.png" alt="1.1.23" title="1.1.23"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub-install /dev/sda</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.24.png" alt="1.1.24" title="1.1.24"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub-mkconfig -o /boot/grub/grub.cfg</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.25.png" alt="1.1.25" title="1.1.25"></p>
<h4 id="退出-卸载-重启">退出，卸载，重启</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br><span class="line">umount -R /mnt</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure>
<p>到这一步差不多就完成了系统的安装，下面就要开始配置系统了。<br>
<img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/1.1.26.png" alt="1.1.26" title="1.1.26"></p>
<p><s>Last Edit Time: 2025-10-23 21:02</s></p>
<h1>配置Arch Linux</h1>
<p>确保以下内容是重启之后所运行的，这里使用root用户登录即可。</p>
<h2 id="网络基础服务：">网络基础服务：</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start iwd.service</span><br><span class="line">systemctl <span class="built_in">enable</span> iwd.service</span><br><span class="line">systemctl start systemd-resolved.service</span><br><span class="line">systemctl <span class="built_in">enable</span> systemd-resolved.service</span><br><span class="line">systemctl <span class="built_in">enable</span> bluetooth.service</span><br><span class="line">systemctl <span class="built_in">enable</span> NetworkManager</span><br><span class="line">systemctl <span class="built_in">enable</span> dhcpcd</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.1.1.png" alt="2.1.1" title="2.1.1"></p>
<h3 id="网络配置">网络配置</h3>
<p><code>iwd 设定</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/iwd/main.conf</span><br></pre></td></tr></table></figure>
<p>写入如下内容</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[General]</span><br><span class="line">EnableNetworkConfiguration=<span class="literal">true</span></span><br><span class="line">NameResolvingService=systemd</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.1.2.png" alt="2.1.2" title="2.1.2"></p>
<p><code>Network Manager设定</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/NetworkManager/NetworkManager.conf</span><br></pre></td></tr></table></figure>
<p>写入如下内容</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[device]</span><br><span class="line">wifi.backend=iwd</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.1.3.png" alt="2.1.3" title="2.1.3"></p>
<h3 id="重启以取得网络">重启以取得网络</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<h4 id="检查是否连接到网路">检查是否连接到网路</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -Syu</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.1.4.png" alt="2.1.4" title="2.1.4"></p>
<p>这个命令只是为了检查有没有得到网络，如果报错，那就说明网络配置有问题，仔细检查一下。<br>
如果没有问题，可以选择升级，也可以选择不升级，完全看自己。</p>
<h2 id="安装cpu编码">安装CPU编码</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S intel-ucode</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S amd-ucode</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.1.5.png" alt="2.1.5" title="2.1.5"></p>
<blockquote>
<p>根据你的CPU，来确定安装什么版本的编码。</p>
</blockquote>
<h2 id="安装显卡驱动">安装显卡驱动</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S xf86-video-intel xf86-video-amdgpu mesa</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.1.6.png" alt="2.1.6" title="2.1.6"></p>
<blockquote>
<p>这里不涉及nVidia显卡的驱动安装过程，如果可以，我会在后面写一篇文章。<br>
<code>xf86-video-intel</code>  Intel核芯显卡<br>
<code>xf86-video-amdgpu</code> AMD显卡<br>
<code>mesa</code> 是为了更好的兼容这两者(?)</p>
</blockquote>
<h2 id="安装声卡驱动">安装声卡驱动</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S pipewire alsa-utils pipewire-pulse pipewire-jack pipewire-alsa</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.1.7.png" alt="2.1.7" title="2.1.7"></p>
<h2 id="增加普通用户">增加普通用户</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">useradd -m -G wheel -s /bin/bash username</span><br><span class="line">passwd username</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.1.8.png" alt="2.1.8" title="2.1.8"></p>
<p>用 <code>passwd username</code> 设定用户密码，输入一个你想要的密码就可以。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/sudoers</span><br></pre></td></tr></table></figure>
<p>修改sudoer以此让<code>romi</code>有使用sudo的权限。<br>
取消 <code>%wheel ALL=(ALL:ALL) ALL</code> 这条语句的注释，也就是删去语句前的 <code>#</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Uncomment to allow members of group wheel to execute any command</span></span><br><span class="line"><span class="comment"># %wheel ALL=(ALL:ALL) ALL</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.1.9.png" alt="2.1.9" title="2.1.9"></p>
<h2 id="配置archlinuxcn源">配置archlinuxcn源</h2>
<p>archlinuxcn有很多有用的软件，十分实用：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/pacman.conf</span><br></pre></td></tr></table></figure>
<p>在最后插入以下内容:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[archlinuxcn]</span><br><span class="line">Server = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/<span class="variable">$arch</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/fix_2.1.10.png" alt="2.1.10" title="2.1.10"></p>
<p>然后安装 <code>archlinuxcn-keyring</code> 防止签名出现问题</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -Sy</span><br><span class="line">pacman -S archlinuxcn-keyring</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/fix_2.1.11.png" alt="2.1.11" title="2.1.11"></p>
<h2 id="安装yay">安装yay</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S yay</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/fix_2.1.12.png" alt="2.1.12" title="2.1.12"></p>
<h2 id="重启">重启</h2>
<p>到这里，基本上所有系统的配置就完成了。下一次登陆系统就是一个完全可以用的Arch Linux系统了。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<h1>桌面配置</h1>
<blockquote>
<p>为了方便，以下使用 <code>root</code> 用户完成</p>
</blockquote>
<p>如果是简约党，为了简单，我就选择<code>i3wm</code></p>
<h2 id="首先安装桌面服务">首先安装桌面服务</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S xorg</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.2.1.png" alt="2.2.1" title="2.2.1"></p>
<h2 id="如果是安装kde">如果是安装<code>KDE</code></h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S plasma sddm konsole dolphin kate ark</span><br></pre></td></tr></table></figure>
<blockquote>
<p>安装好后设定<code>sddm</code>的自启动</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> sddm</span><br></pre></td></tr></table></figure>
<h2 id="如果是安装gnome">如果是安装<code>Gnome</code></h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S gnome</span><br></pre></td></tr></table></figure>
<blockquote>
<p>安装好后设定<code>gdm</code>的自启动</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> gdm</span><br></pre></td></tr></table></figure>
<h2 id="如果是安装i3wm">如果是安装<code>i3wm</code></h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pacman -S xorg-xinit xorg-server i3-gaps</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/Archlinux-U%E7%9B%98-%E6%89%93%E9%80%A0%E9%9A%8F%E8%BA%AB%E7%B3%BB%E7%BB%9F%E7%9B%98/2.2.2.png" alt="2.2.2" title="2.2.2"></p>
<p><s>Last Edit Time: 2025-11-03 22:17</s></p>
<h1>系统美化</h1>
<h1>未完</h1>
<div style="text-align: center;">在最后的最后，感恩。</div>
<p align="right">Romi Brooks♥</p>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Archlinux</tag>
        <tag>Installation</tag>
        <tag>i3wm</tag>
      </tags>
  </entry>
  <entry>
    <title>Navidrome，定义你自己离线的播放器</title>
    <url>/2023/03/04/Navidrome%EF%BC%8C%E5%AE%9A%E4%B9%89%E4%BD%A0%E8%87%AA%E5%B7%B1%E7%9A%84%E7%A6%BB%E7%BA%BF%E6%92%AD%E6%94%BE%E5%99%A8/</url>
    <content><![CDATA[<p><img src="/images/posts/Navidrome-U-Own-Music-Player/cover.png" alt="封面" title="Cover"></p>
<p>最近在上传我的本地歌曲库的时候,发现过于麻烦,(筛选,打包,上传.如果我需要修改某个文件的时候还需要再下载回来,然后解压,再修改.重复这个步骤)于是想到了托管到VPS,并且实现流媒体播放.那么这还是蛮不错的!</p>
<blockquote>
<p>PS:这个很早就打算做了,只不过懒,就一直拖下去了(因为早期下载歌曲的时候过于偷懒导致过50%的歌曲的metadata都出现了不匹配的问题…)</p>
</blockquote>
<span id="more"></span>  
<p><strong>同样,这个东西也适用于内网用户(如NAS用户)访问自己的媒体文件,其实这类软件目标人群应该就是这个.</strong></p>
<h1>寻找一个适合的软件</h1>
<p>我的基本要求是,颜值首先!如果不好看,我估计我会第一个舍弃…<br>
其次必须支持<code>流式媒体播放</code>,这一次托管的目的就在这里!<br>
然后是功能必须齐全:比如<code>歌手排序</code>,<code>歌单排序</code>,<code>用户喜爱</code>,<code>封面展示</code>…<br>
最后是方便部署,如果部署起来特别麻烦,那是真的得不偿失…<br>
我大概花费了几个小时尝试,而后偶然发现了<code>Navidrome</code>,也就是这篇文章的主角.</p>
<h1>部署</h1>
<p><code>Navidrome</code> 的部署分为三步,我觉得简单到天际…</p>
<ul>
<li>下载,解压</li>
<li>配置歌曲文件夹</li>
<li>运行</li>
</ul>
<p>Step 1. 下载只需要到<a href="https://github.com/navidrome/navidrome">Navidrome</a>的<a href="https://github.com/navidrome/navidrome/releases">releases</a>里下符合你系统架构的二进制文件. ez to do it.<br>
然后解压,一般直接</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -xvf filename.tar.gz</span><br></pre></td></tr></table></figure>
<p>Step 2. 在你程序目录新建一个<code>music</code>文件夹,可以直接放音乐,也可以软连接到你自己的音乐文件夹.只要有音乐就行!<br>
Step 3. 启动,这里推荐用<code>screen</code>或者是你喜欢的什么软件来防止被ban.到这里<code>Navidrome</code>就部署完成了.简单的一b</p>
<p>后续的设置基本就是<code>Nginx 反向代理</code>,让他更加简洁美观 (同时加锁,即取得https)</p>
<p>参考上一篇博客,在<code>player.ssl.conf</code>里写入以下内容(根据自己的改):</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line">server&#123;</span><br><span class="line">    <span class="attribute">listen</span>  <span class="number">443</span> ssl;</span><br><span class="line">    <span class="attribute">ssl_certificate</span> /etc/letsencrypt/live/player.romichan.me/fullchain.pem;</span><br><span class="line">    <span class="attribute">ssl_certificate_key</span> /etc/letsencrypt/live/player.romichan.me/privkey.pem;</span><br><span class="line">    <span class="attribute">server_name</span>  player.romichan.me;</span><br><span class="line"></span><br><span class="line">    <span class="section">location</span> / &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span>  http://127.0.0.1:4533; <span class="comment"># 这里就是转发的地址了</span></span><br><span class="line">        <span class="attribute">proxy_set_header</span> Host <span class="variable">$proxy_host</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>player.conf</code>里写入:</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span>  <span class="number">80</span>;</span><br><span class="line">    <span class="attribute">server_name</span>  player.romichan.me;</span><br><span class="line">    <span class="attribute">rewrite</span><span class="regexp"> ^(.*)$</span>  https://<span class="variable">$host</span><span class="variable">$1</span> <span class="literal">permanent</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>最后</h1>
<p>到这里,我认为全部就部署好了<br>
这时候使用<code>player.romichan.me</code>进入你的<code>Navidrome</code>,初次使用会让你设置管理员用户和密码,记好就行了.</p>
<p>还有什么要说的呢?<br>
哦,对…<br>
你在使用这些离线音乐库的时候就要注意,你本地音乐的metadata一定不要有太多的问题,不然整个页面看起来都会很乱<br>
我为什么会在文章开头我懒得去弄呢?就是因为我后续整理metadata用了将近三个小时…这真的很折磨…<br>
<img src="/images/posts/Navidrome-U-Own-Music-Player/1.0.1.jpg" alt="折磨" title="折磨"></p>
<p>~当然如果你的习惯很好,那么这个当然对你来说没有任何问题,加油!~</p>
<p align="right">在最后的最后，感恩</p>
<p align="right">Romi Brooks♥</p>]]></content>
      <categories>
        <category>Music</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Website</tag>
        <tag>Music</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch 张量计算</title>
    <url>/2025/11/07/Pytorch%E5%BC%A0%E9%87%8F%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h1>什么是 Tensor?</h1>
<p>张量（Tensor）是物理的语言，在力学和广义相对论等领域发挥着不可替代的作用。但是在深度学习中，张量是基础数据结构，可以看作是多维数据容器。</p>
<p>现代深度学习框架（如 TensorFlow、PyTorch）的核心数据结构就是张量，所有模型运算（卷积、全连接、注意力机制）本质上都是张量操作：</p>
<ul>
<li>卷积神经网络（CNN）通过二维张量卷积提取图像局部特征。</li>
<li>Transformer 模型利用张量的多头注意力机制处理序列数据的长距离依赖。</li>
<li>张量分解（如 CP 分解、Tucker 分解）可用于模型压缩、特征降维，解决高维数据的 “维度灾难” 问题。</li>
</ul>
<p>学习张量计算能帮助理解深度学习的底层数学逻辑，而非仅停留在 API 调用层面。</p>
<blockquote>
<p>Attention: 本文假设读者拥有使用Python的经验，不对基础语法进行解释, 在教学部分使用英文教学。</p>
</blockquote>
<span id="more"></span> 
<h1>为何选择 PyTorch?</h1>
<p>作为主流深度学习框架，PyTorch 以简洁易用、动态计算图特性著称，兼顾科研灵活性与工业落地能力：</p>
<ul>
<li>对初学者友好，API 设计直观，能快速上手搭建神经网络。</li>
<li>生态丰富，覆盖计算机视觉、自然语言处理、强化学习等多领域，适配科研实验与实际项目开发。</li>
<li>深度集成 Python 生态，支持即时调试，大幅降低模型开发与优化成本。</li>
</ul>
<h1>PyTorch的安装与引入</h1>
<h2 id="安装pytorch">安装PyTorch:</h2>
<p>首先进入PyTorch官网中的<a href="https://pytorch.org/get-started/locally/">GetStart</a>页面，我们通过pip包管理器来安装PyTorch。<br>
对于版本，选择Stable版本即可，通常来说系统的选择不影响pip的安装，所以我们不做解释，对于语言我们选择Python语言，而版本，选择最新即可。<br>
<img src="/images/posts/PyTorch%E5%BC%A0%E9%87%8F%E8%AE%A1%E7%AE%97/torchInstall.png" alt="安装PyTorch"><br>
复制<code>Run this Command</code>下的内容，比如我这里是：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126</span><br></pre></td></tr></table></figure>
<p>这时候打开你的venv环境或者在终端下粘贴即可安装。</p>
<h2 id="引入pytorch">引入PyTorch:</h2>
<p>使用<code>torch.__version__</code>来查看PyTorch的版本。<br>
使用<code>torch.cuda.is_available()</code>来查看当前PyTorch是否支持Cuda计算。<br>
什么是Cuda，我们可以把它理解为一个在GPU上加速计算的一个工具：</p>
<blockquote>
<p>By Nvidia Cuda Zone:<br>
<em>CUDA® is a parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs)</em><br>
<em>With CUDA, developers are able to dramatically speed up computing applications by harnessing the power of GPUs.</em></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>
<p>这将会输出类似的内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2.8.0+cu126</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<h1>开始学习PyTorch</h1>
<h2 id="before-we-begin-our-studies">Before we begin our studies</h2>
<p>This function allows us to quickly get the properties of PyTorch tensors.<br>
We will not explain the purpose of this function in subsequent content.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_tensor_info</span>(<span class="params">p_type : <span class="built_in">str</span>, p_tensor : torch.Tensor</span>) :</span><br><span class="line">    <span class="built_in">print</span>(p_type, <span class="string">&quot;Tensor belong&quot;</span>, p_tensor.ndim, <span class="string">&quot;Dimension with Data Type&quot;</span>, p_tensor.dtype, <span class="string">&quot;On&quot;</span>, p_tensor.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;and the Shape of&quot;</span>, p_type, <span class="string">&quot;is&quot;</span>, p_tensor.shape)</span><br></pre></td></tr></table></figure>
<h2 id="pytorch-tensor-unit">PyTorch Tensor Unit</h2>
<p>Before we continue, we need to understand PyTorch’s <code>Dimension</code> and <code>Shape</code>.<br>
It is <strong>not</strong> a comprehensive data structure reference, as tensors may also have other data structures.<br>
Therefore, we only present the key properties of tensors relevant to this chapter.</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Tensor Dimension</strong></td>
<td>Return this tensor’s dimension</td>
<td><code>tensor.ndim</code></td>
</tr>
<tr>
<td></td>
<td>Scalar: 0 Dim</td>
<td><code>torch.tensor(5).ndim</code> -&gt; 0</td>
</tr>
<tr>
<td></td>
<td>Vector: 1 Dim</td>
<td><code>torch.tensor([1,2,3]).ndim</code> -&gt; 1</td>
</tr>
<tr>
<td></td>
<td>Matrix: 2 Dim</td>
<td><code>torch.tensor([[1,2],[3,4]]).ndim</code> -&gt; 2</td>
</tr>
<tr>
<td><strong>Tensor Shape</strong></td>
<td>Return this tensor’s shape (size)</td>
<td><code>tensor.shape</code> or <code>tensor.size()</code></td>
</tr>
<tr>
<td></td>
<td>Scalar: will return <code>torch.size([])</code></td>
<td><code>torch.tensor(7).shape</code> -&gt; torch.size([])</td>
</tr>
<tr>
<td></td>
<td>Vector: will return <code>length in this vector</code></td>
<td><code>torch.tensor([2,3]).shape</code> -&gt; torch.size([2])</td>
</tr>
<tr>
<td></td>
<td>Matrix: will return <code>items in this matrix and length in item</code></td>
<td><code>torch.tensor([[1,2,3],[4,5,6]].shape</code> -&gt; torch.size([2,3])</td>
</tr>
</tbody>
</table>
<h2 id="get-a-tensor">Get a Tensor</h2>
<p><strong>How to get a Tensor, and what is the Tensor’s Data Type?</strong><br>
Use <code>torch.tensor()</code> to get a Tensor<br>
Use <code>tensor.dtype</code> to get tensor’s type<br>
Use <code>tensor.shape</code> to get tensor’s shape<br>
Use <code>tensor.device</code> to get tensor’s device</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">int_tensor = torch.tensor(<span class="number">1</span>) <span class="comment"># Get a tensor</span></span><br><span class="line">get_tensor_info(<span class="string">&quot;int_tensor&quot;</span>, int_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Or Specify the type explicitly</span></span><br><span class="line">float32_tensor = torch.tensor(<span class="number">1.0</span>, dtype=torch.float32)</span><br><span class="line">get_tensor_info(<span class="string">&quot;float32_tensor&quot;</span>, float32_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Explicit type conversion</span></span><br><span class="line">float16_tensor = float32_tensor.<span class="built_in">type</span>(torch.float16)</span><br><span class="line">get_tensor_info(<span class="string">&quot;float16_tensor&quot;</span>, float16_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># More additional condition</span></span><br><span class="line">condition_tensor = torch.tensor([<span class="number">3.0</span>, <span class="number">6.0</span>, <span class="number">8.0</span>],</span><br><span class="line">                                dtype=torch.float32, <span class="comment"># Data type for this tensor</span></span><br><span class="line">                                device=torch.device(<span class="string">&quot;cuda:0&quot;</span>), <span class="comment"># Put this tensor to where ?</span></span><br><span class="line">                                requires_grad=<span class="literal">True</span>) <span class="comment"># Is Track the gradients with this tensor operation ?</span></span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int_tensor Tensor belong 0 Dimension with Data Type torch.int64 On cpu</span><br><span class="line">and the Shape of int_tensor is torch.Size([])</span><br><span class="line">float32_tensor Tensor belong 0 Dimension with Data Type torch.float32 On cpu</span><br><span class="line">and the Shape of float32_tensor is torch.Size([])</span><br><span class="line">float16_tensor Tensor belong 0 Dimension with Data Type torch.float16 On cpu</span><br><span class="line">and the Shape of float16_tensor is torch.Size([])</span><br></pre></td></tr></table></figure>
<h2 id="scalar-tensor">Scalar Tensor</h2>
<p><strong>What is the <code>Scalar Tensor</code>:</strong><br>
A scalar is a zero-dimensional tensor representing a single numerical value (such as an integer or floating-point number) without spatial dimensions (ndim=0).<br>
It constitutes one of the most fundamental units within PyTorch’s tensor system, enabling seamless operations with higher-dimensional tensors (vectors, matrices, etc.).</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Scalar</span></span><br><span class="line">scalar = torch.tensor(<span class="number">7</span>)</span><br><span class="line"><span class="comment"># scalar.item() # get that tensor as the python data</span></span><br><span class="line">get_tensor_info(<span class="string">&quot;Scalar&quot;</span>, scalar)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Scalar Tensor belong 0 Dimension with Data Type torch.int64 On cpu</span><br><span class="line">and the Shape of Scalar is torch.Size([])</span><br></pre></td></tr></table></figure>
<h2 id="vector-tensor">Vector Tensor</h2>
<p><strong>What is the <code>Vector Tensor</code>:</strong><br>
A Vector Tensor is a one-dimensional tensor (1-dimensional tensor) corresponding to the mathematical concept of a “vector”.<br>
It consists of an ordered sequence of numerical values and possesses only one dimension (ndim=1).<br>
It serves as an extension of the scalar (zero-dimensional) and forms the foundation for constructing higher-dimensional tensors such as matrices and 3D tensors.</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Vector</span></span><br><span class="line">vector = torch.tensor([<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">get_tensor_info(<span class="string">&quot;Vector&quot;</span>, vector)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Vector Tensor belong 1 Dimension with Data Type torch.int64 On cpu</span><br><span class="line">and the Shape of Vector is torch.Size([2])</span><br></pre></td></tr></table></figure>
<h2 id="matrix-tensor">Matrix Tensor</h2>
<p><strong>What is the <code>Matrix Tensor</code>:</strong><br>
A Matrix Tensor is a two-dimensional tensor corresponding to the mathematical concept of a “matrix”.<br>
It is a two-dimensional numerical array composed of <strong>rows and columns</strong>, possessing a dimension of 2 (ndim=2).</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Matrix</span></span><br><span class="line">matrix = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">get_tensor_info(<span class="string">&quot;Matrix&quot;</span>, matrix)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Matrix Tensor belong 2 Dimension with Data Type torch.int64 On cpu</span><br><span class="line">and the Shape of Matrix is torch.Size([2, 3])</span><br></pre></td></tr></table></figure>
<h2 id="what-is-the-tensor-s-shape">What is the Tensor’s Shape</h2>
<p>We will give an E.g. Tensor:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Tensor</span></span><br><span class="line">tensor = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">                        [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">                        [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]]])</span><br><span class="line">get_tensor_info(<span class="string">&quot;Tensor&quot;</span>, tensor)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tensor Tensor belong 3 Dimension with Data Type torch.int64 On cpu</span><br><span class="line">and the Shape of Tensor is torch.Size([1, 3, 3])</span><br></pre></td></tr></table></figure>
<p><strong>Therefore, in PyTorch, the shape of a tensor signifies:</strong><br>
A Tensor’s Shape refers to the number of elements across each dimension of the tensor.<br>
It is represented as a tuple (of type torch.Size, a subclass of tuple) to describe the structure of the tensor (such as “how many rows, how many columns, how many channels”, etc.).</p>
<p>But why the tensor’s shape is torch.Size([1,3,3]) here:<br>
1: Meaning that for the 3Dim tensor, the outer [] layer only contain one [] item<br>
2: the middle [] layer contain three [] item<br>
3: the inner [] layer contain three element item.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t_tensor = torch.tensor(</span><br><span class="line">[                     <span class="comment"># ← 第1层括号 → 第1维度（大小=1）</span></span><br><span class="line">  [                   <span class="comment"># ← 第2层括号 → 第2维度（大小=3）</span></span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],        <span class="comment"># ← 第3层括号 → 第3维度（大小=3）</span></span><br><span class="line">    [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">    [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">  ]</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 id="radom-tensor">Radom Tensor</h2>
<p><strong>Why Random Tensor:</strong><br>
Fundamentally, random numbers are indispensable in scenarios such as model training, algorithm validation, and data augmentation.<br>
Amidst the continuous updating of data, our ultimate objective is to arrive at a result that meets our requirements.<br>
Random numbers provide us with a starting point, thereby ensuring that computations commence correctly.</p>
<blockquote>
<p>just like:<br>
Start with random numbers -&gt; look at data -&gt; update random numbers -&gt; look at data -&gt; … -&gt; Expected data</p>
</blockquote>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_tensor = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(random_tensor)</span><br><span class="line">get_tensor_info(<span class="string">&quot;Random&quot;</span>, random_tensor)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.0503,  0.0958, -0.6131, -0.2491],</span><br><span class="line">        [ 0.0630,  0.0489,  0.4874, -1.5149],</span><br><span class="line">        [-0.0070, -1.4396, -0.1872, -0.0911]])</span><br><span class="line">Random Tensor belong 2 Dimension with Data Type torch.float32 On cpu</span><br><span class="line">and the Shape of Random is torch.Size([3, 4])</span><br></pre></td></tr></table></figure>
<p><br>Here is one example of the application of random tensors:<br>
In the field of image processing, we can utilise a randomised noise image to approximate the target image.<br>
Through this code, we can generate a 224×224 RGB-channel image tensor.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a Radom Tensor with similar shape to an image tensor</span></span><br><span class="line">random_image_tensor = torch.randn(size=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)) <span class="comment"># 224 height, 224 width, 3 color (R,G,B)</span></span><br><span class="line">get_tensor_info(<span class="string">&quot;Random Image&quot;</span>, random_image_tensor)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Random Image Tensor belong 3 Dimension with Data Type torch.float32 On cpu</span><br><span class="line">and the Shape of Random Image is torch.Size([224, 224, 3])</span><br></pre></td></tr></table></figure>
<p><strong>An interesting little experiment</strong><br>
Cool tools, trans the Random Image Tensor to the img</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤1：确保数据类型正确（转换为float32）</span></span><br><span class="line"><span class="keyword">if</span> random_image_tensor.dtype != torch.float32:</span><br><span class="line">    image_tensor = random_image_tensor.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤2：归一化像素值到 [0, 1] 范围（如果尚未归一化）</span></span><br><span class="line"><span class="keyword">if</span> random_image_tensor.<span class="built_in">max</span>() &gt; <span class="number">1.0</span>:</span><br><span class="line">    image_tensor = random_image_tensor / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤3：转换为NumPy数组并调整通道顺序</span></span><br><span class="line">image_np = random_image_tensor.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤4：使用Matplotlib显示图像</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line">plt.imshow(image_np)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)  <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/posts/PyTorch%E5%BC%A0%E9%87%8F%E8%AE%A1%E7%AE%97/randomPic.png" alt="What does a random tensor look like?"></p>
<h2 id="zero-tensor-and-one-tensor">Zero Tensor and One Tensor</h2>
<p>Fundamentally: any tensor with all elements set to zero is termed a Zero Tensor, while any tensor with all elements set to one is termed a One Tensor.<br>
They serve numerous purposes, some of which may be quite advanced. However, at their simplest, Zero Tensors are typically employed for parameter initialisation, whereas One Tensors are generally used for element-wise operations and normalisation.</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">zero_tensor = torch.zeros(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(zero_tensor)</span><br><span class="line"></span><br><span class="line">one_tensor = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(one_tensor)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0.]])</span><br><span class="line">tensor([[1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1.]])</span><br></pre></td></tr></table></figure>
<h2 id="range-of-tensor">Range of Tensor</h2>
<p>Occasionally, we may need to generate tensors within a specific range or with a specific step.<br>
This can be achieved as follows:<br>
E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">range_tensor = torch.arange(<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(range_tensor)</span><br><span class="line"><span class="comment"># Or addition condition</span></span><br><span class="line">range_addition_tensor = torch.arange(start=<span class="number">0</span>, end=<span class="number">10</span>, step=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(range_addition_tensor)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span><br><span class="line">tensor([0, 2, 4, 6, 8])</span><br></pre></td></tr></table></figure>
<h2 id="tensors-like">Tensors-Like</h2>
<p>Tensors-Like enables you to swiftly create a new tensor based on the shape, data type, and device of an existing <code>reference tensor</code>, without requiring manual specification of these attributes.<br>
E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Tensors-Like</span></span><br><span class="line"><span class="comment"># Create a tensor like what you choose tensor</span></span><br><span class="line">zero_like_tensor = torch.zeros_like(range_tensor)</span><br><span class="line"><span class="built_in">print</span>(zero_like_tensor)</span><br></pre></td></tr></table></figure>
<p>This will create a new vector with the same properties as the <code>range_tensor</code> attribute from the previous section <a href="#range-of-tensor">Range of Tensor</a>, without requiring us to redefine the attribute.</p>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</span><br></pre></td></tr></table></figure>
<h2 id="tensor-s-operations">Tensor’s Operations</h2>
<p><strong>Make sure that calculate tensor with this:</strong></p>
<ul>
<li>The right Data Type</li>
<li>The right Device</li>
<li>The right Shape (like: float16 tensor * float16 tensor)</li>
</ul>
<p><em>The rules governing tensor operations bear a striking resemblance to those of matrix operations.<br>
Should you have studied linear algebra, you will be thoroughly familiar with them.</em></p>
<p>The following code examples illustrate the expressions for <code>addition</code>, <code>multiplication</code>, and <code>subtraction</code> operations on a tensor, along with their results, enabling us to grasp these concepts quickly.</p>
<blockquote>
<p><em>Please note that we have not included <code>element-wise division</code> or <code>inverse operations on two-dimensional tensors (i.e., matrices)</code>.</em></p>
</blockquote>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">op_tensor = torch.tensor([<span class="number">1.5</span>,<span class="number">2.4</span>,<span class="number">6.9</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor:&quot;</span>, op_tensor)</span><br><span class="line"></span><br><span class="line">op_plus_tensor = op_tensor + <span class="number">10</span> <span class="comment"># Or use: op_plus_tensor = torch.add(op_tensor, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Plus 10 to tensor:&quot;</span>, op_plus_tensor)</span><br><span class="line"></span><br><span class="line">op_mul_tensor = op_tensor * <span class="number">10</span> <span class="comment"># Or use: op_mul_tensor = torch.mul(op_tensor, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mul 10 to tensor:&quot;</span>, op_mul_tensor)</span><br><span class="line"></span><br><span class="line">op_sub_tensor = op_tensor - <span class="number">10</span> <span class="comment"># Or use: op_sub_tensor = torch.sub(op_tensor, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sub 10 to tensor:&quot;</span>, op_sub_tensor)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tensor: tensor([1.5000, 2.4000, 6.9000])</span><br><span class="line">Plus 10 to tensor: tensor([11.5000, 12.4000, 16.9000])</span><br><span class="line">Mul 10 to tensor: tensor([15., 24., 69.])</span><br><span class="line">Sub 10 to tensor: tensor([-8.5000, -7.6000, -3.1000])</span><br></pre></td></tr></table></figure>
<h3 id="conditions-for-matrix-multiplication">Conditions for Matrix Multiplication</h3>
<p>If you have not studied linear algebra, here is a brief introduction to the rules governing matrix multiplication.<br>
<strong>Dimensional representation</strong></p>
<ul>
<li>Let the dimensions of matrix <strong>A</strong> be <strong>m × n</strong>.</li>
<li>Let the dimensions of matrix <strong>B</strong> be <strong>p × q</strong>.</li>
<li><strong>Conditions for multiplication</strong>: n = p</li>
</ul>
<p><strong>Dimensional Properties</strong></p>
<ul>
<li>Number of rows in C = Number of rows in A</li>
<li>Number of columns in C = Number of columns in B</li>
<li>Dimension formula: (m × n) · (n × p) = (m × p)</li>
</ul>
<p><strong>Example Explanation</strong></p>
<table>
<thead>
<tr>
<th>Matrix A Dimensions</th>
<th>Matrix B Dimensions</th>
<th>Can Be Multiplied</th>
<th>Result Dimensions</th>
</tr>
</thead>
<tbody>
<tr>
<td>2×3</td>
<td>3×4</td>
<td>✅ Yes</td>
<td>2×4</td>
</tr>
<tr>
<td>4×2</td>
<td>3×4</td>
<td>❌ No</td>
<td>-</td>
</tr>
<tr>
<td>5×5</td>
<td>5×1</td>
<td>✅ Yes</td>
<td>5×1</td>
</tr>
<tr>
<td>3×2</td>
<td>2×3</td>
<td>✅ Yes</td>
<td>3×3</td>
</tr>
</tbody>
</table>
<p><strong>Matrix multiplication comprises two types: Cross multiplication and Dot multiplication:</strong><br>
Dot multiplication: The dot product of two vectors is obtained by multiplying corresponding elements and summing the results, yielding a scalar quantity denoted as <strong>a·b</strong>.<br>
Cross multiplication: The cross product of two three-dimensional vectors is a new vector, denoted as <strong>axb</strong>.</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Wise multiplication axb (aka. Cross Multiplication)</span></span><br><span class="line"><span class="comment"># Simply use the * operator</span></span><br><span class="line">op_wisemul_tensor = op_tensor * op_tensor</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CrossMul tensor x tensor:&quot;</span>, op_wisemul_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix Multiplication a.b (aka. Dot Product)</span></span><br><span class="line">op_matmul_tensor = torch.matmul(op_tensor, op_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MatMul tensor . tensor:&quot;</span>, op_matmul_tensor)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CrossMul tensor x tensor: tensor([ 2.2500,  5.7600, 47.6100])</span><br><span class="line">MatMul tensor . tensor: tensor(55.6200)</span><br></pre></td></tr></table></figure>
<h2 id="matrix-transpose">Matrix Transpose</h2>
<p>Matrix transposition involves swapping the rows and columns of a matrix.<br>
Its primary function is to rearrange the dimensions of the matrix to accommodate computational requirements or align with data formats.</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Use tensor.T to get this matrix&#x27;s Transpose matrix</span></span><br><span class="line">before_tensor = torch.tensor([[<span class="number">1.5</span>,<span class="number">2.4</span>,<span class="number">6.9</span>],</span><br><span class="line">                             [<span class="number">2.4</span>,<span class="number">8.3</span>,<span class="number">9.2</span>]])</span><br><span class="line">after_tensor = before_tensor.T</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Before Transpose:\n&quot;</span>, before_tensor, <span class="string">&quot;\nShape:&quot;</span>, before_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After Transpose:\n&quot;</span>, after_tensor, <span class="string">&quot;\nShape:&quot;</span>, after_tensor.shape)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Before Transpose:</span><br><span class="line"> tensor([[1.5000, 2.4000, 6.9000],</span><br><span class="line">        [2.4000, 8.3000, 9.2000]]) </span><br><span class="line">Shape: torch.Size([2, 3])</span><br><span class="line">After Transpose:</span><br><span class="line"> tensor([[1.5000, 2.4000],</span><br><span class="line">        [2.4000, 8.3000],</span><br><span class="line">        [6.9000, 9.2000]]) </span><br><span class="line">Shape: torch.Size([3, 2])</span><br></pre></td></tr></table></figure>
<h2 id="tensor-aggregation">Tensor Aggregation</h2>
<p>For data analysis, simplifying data, or summarising statistical characteristics, we can use tensor aggregation operations: <code>minimum</code>, <code>maximum</code>, <code>mean</code> aka. avg, and <code>sum</code>.</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">agg_tensor = torch.tensor([<span class="number">1.5</span>,<span class="number">2.4</span>,<span class="number">6.9</span>,<span class="number">42</span>,<span class="number">6.4</span>,<span class="number">25</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(agg_tensor)</span><br><span class="line">agg_min_tensor = agg_tensor.<span class="built_in">min</span>() <span class="comment"># Or use: agg_min_tensor = torch.min(agg_tensor)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor&#x27;s Min elem:&quot;</span>, agg_min_tensor)</span><br><span class="line">agg_max_tensor = agg_tensor.<span class="built_in">max</span>() <span class="comment"># Or use: agg_max_tensor = torch.max(agg_tensor)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor&#x27;s Max elem:&quot;</span>, agg_max_tensor)</span><br><span class="line">agg_avg_tensor = agg_tensor.mean() <span class="comment"># Or use: agg_avg_tensor = torch.mean(agg_tensor)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor&#x27;s Mean(Avg):&quot;</span>, agg_avg_tensor)</span><br><span class="line">agg_sum_tensor = agg_tensor.<span class="built_in">sum</span>() <span class="comment"># Or use: agg_sum_tensor = torch.sum(agg_tensor)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor&#x27;s Sum:&quot;</span>, agg_sum_tensor)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([ 1.5000,  2.4000,  6.9000, 42.0000,  6.4000, 25.0000,  1.0000])</span><br><span class="line">Tensor&#x27;s Min elem: tensor(1.)</span><br><span class="line">Tensor&#x27;s Max elem: tensor(42.)</span><br><span class="line">Tensor&#x27;s Mean(Avg): tensor(12.1714)</span><br><span class="line">Tensor&#x27;s Sum: tensor(85.2000)</span><br></pre></td></tr></table></figure>
<h2 id="tensor-s-item-position">Tensor’s item position</h2>
<p>Indexing corresponding values in a tensor, such as retrieving the indices (positions) of the minimum and maximum values in the following code:<br>
E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Find min item index</span></span><br><span class="line">min_pos = agg_tensor.argmin()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor&#x27;s minimal item&quot;</span>, agg_tensor[min_pos], <span class="string">&quot;with index:&quot;</span>, min_pos)</span><br><span class="line">max_pos = agg_tensor.argmax()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor&#x27;s maximum item&quot;</span>, agg_tensor[max_pos], <span class="string">&quot;with index:&quot;</span>, max_pos)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tensor&#x27;s minimal item tensor(1.) with index: tensor(6)</span><br><span class="line">Tensor&#x27;s maximum item tensor(42.) with index: tensor(3)</span><br></pre></td></tr></table></figure>
<h2 id="reshape">Reshape</h2>
<p>Reshape: By adjusting the dimensionality of a tensor, its shape is altered whilst preserving the total number of elements, thereby reorganising its dimensions.</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Reshape the input tensor to a defined shape</span></span><br><span class="line">x_tensor = torch.arange(<span class="number">0</span>, <span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line"><span class="built_in">print</span>(x_tensor, x_tensor.shape)</span><br><span class="line">x_reshape_tensor = x_tensor.reshape(<span class="number">2</span>,<span class="number">5</span>) <span class="comment"># tensor.reshape(n,m) -&gt; n*m must be equal to torch size</span></span><br><span class="line"><span class="built_in">print</span>(x_reshape_tensor)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) torch.Size([10])</span><br><span class="line">tensor([[0., 1., 2., 3., 4.],</span><br><span class="line">        [5., 6., 7., 8., 9.]])</span><br></pre></td></tr></table></figure>
<h2 id="view">View</h2>
<p>Its core functionality is similar to <code>reshape()</code> — adjusting the dimensionality of a tensor while preserving the total number of elements — but it imposes a crucial restriction: it is only applicable to “memory-contiguous” tensors.<br>
If, after creating a tensor, its memory is contiguous, then <code>view()</code> can be used immediately.<br>
However, if other operations are performed, such as transposition or permuting, once the tensor becomes non-contiguous, <code>view()</code> cannot be used. Instead, <code>reshape()</code> must be employed.</p>
<p>Concurrently, another interpretation is that this bears some resemblance to references in C++.<br>
For instance, in the following code, we convert <code>x_tensor</code> into a <code>view_tensor</code>. Thereafter, our subsequent operations will also affect <code>x_tensor</code>.</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_view_tensor = x_tensor.view(<span class="number">1</span>,<span class="number">10</span>) <span class="comment"># now x_view_tensor linked with x_tensor</span></span><br><span class="line"><span class="built_in">print</span>(x_view_tensor)</span><br><span class="line"></span><br><span class="line">x_view_tensor[:,<span class="number">0</span>] = <span class="number">123</span></span><br><span class="line"><span class="built_in">print</span>(x_view_tensor,<span class="string">&quot;\n&quot;</span>, x_tensor) <span class="comment"># also changed the x_tensor</span></span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]])</span><br><span class="line">tensor([[123.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.]]) </span><br><span class="line"> tensor([123.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])</span><br></pre></td></tr></table></figure>
<h2 id="stack-tensor">Stack Tensor</h2>
<p><code>torch.stack()</code> is used to concatenate multiple tensors of the same shape along a newly created dimension.<br>
It adds a new dimension and sequentially ‘stacks’ the input tensors along this new dimension.</p>
<p>The following code creates a tensor with a range from 0 to 4, which we stack horizontally and vertically using <code>torch.stack()</code>:</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Stack tensors together</span></span><br><span class="line">tensor = torch.arange(<span class="number">0</span>, <span class="number">5</span>).<span class="built_in">float</span>()</span><br><span class="line">stack_tensor_H = torch.stack([tensor, tensor, tensor],dim=<span class="number">1</span>)  <span class="comment"># horizontally stack</span></span><br><span class="line">stack_tensor_V = torch.stack([tensor, tensor, tensor],dim=<span class="number">0</span>)  <span class="comment"># vertically stack</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;HStack:\n&quot;</span>, stack_tensor_H, <span class="string">&quot;\nVStack:\n&quot;</span>, stack_tensor_V)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HStack:</span><br><span class="line"> tensor([[0., 0., 0.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [2., 2., 2.],</span><br><span class="line">        [3., 3., 3.],</span><br><span class="line">        [4., 4., 4.]]) </span><br><span class="line">VStack:</span><br><span class="line"> tensor([[0., 1., 2., 3., 4.],</span><br><span class="line">        [0., 1., 2., 3., 4.],</span><br><span class="line">        [0., 1., 2., 3., 4.]])</span><br></pre></td></tr></table></figure>
<h2 id="squeeze-unsqueeze">Squeeze &amp; Unsqueeze</h2>
<p><code>torch.squeeze()</code> is an operation designed to remove all dimensions of size 1 from a tensor.<br>
Its core function is to simplify the tensor structure by eliminating those “superfluous” dimensions containing only a single element (i.e., dimensions of size 1), thereby rendering the tensor shape more compact.</p>
<p>Conversely, <code>torch.unsqueeze()</code> is used to insert a new dimension of size 1 at a specified position (dimension) within a tensor.<br>
This operation does not alter the number of elements in the tensor, merely increasing the number of dimensions.</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Delete the tensor&#x27;s 1 dim</span></span><br><span class="line">z_tensor = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]]).<span class="built_in">float</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Before Squeeze:&quot;</span>, z_tensor, <span class="string">&quot;\n&quot;</span>, z_tensor.shape)</span><br><span class="line">z_squeeze_tensor = torch.squeeze(z_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After Squeeze:&quot;</span>, z_squeeze_tensor, <span class="string">&quot;\n&quot;</span>, z_squeeze_tensor.shape)</span><br><span class="line"><span class="comment"># Unsqueeze</span></span><br><span class="line"><span class="comment"># Add 1 dim to tensor</span></span><br><span class="line">z_unsqueeze_tensor = torch.unsqueeze(z_squeeze_tensor, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After Unsqueeze:&quot;</span>, z_unsqueeze_tensor, <span class="string">&quot;\n&quot;</span>, z_unsqueeze_tensor.shape)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Before Squeeze: tensor([[1., 2., 3., 1., 2., 2.]]) </span><br><span class="line"> torch.Size([1, 6])</span><br><span class="line">After Squeeze: tensor([1., 2., 3., 1., 2., 2.]) </span><br><span class="line"> torch.Size([6])</span><br><span class="line">After Unsqueeze: tensor([[1., 2., 3., 1., 2., 2.]]) </span><br><span class="line"> torch.Size([1, 6])</span><br></pre></td></tr></table></figure>
<h2 id="permute">Permute</h2>
<p><code>torch.permute()</code> is an operation used to rearrange the dimension order of a tensor. We shall illustrate this with a simple example using the code below. Initially, we created a tensor with a resolution of 224×224 (x, y) and an RGB channel (z).<br>
Should we then require the colour channel to be placed at the front, resulting in the order (z, x, y), we can employ <code>permute</code> to swiftly swap the multidimensional data.</p>
<p>E.g.:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_tensor = torch.randn(size=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)) <span class="comment"># 224 height, 224 width, 3 color (R,G,B)</span></span><br><span class="line">permute_img_tensor = img_tensor.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>) <span class="comment"># Shift to 3 color, 224 height, 224 width</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Permute(Shift)&quot;</span>, img_tensor.shape, <span class="string">&quot;to&quot;</span>, permute_img_tensor.shape)</span><br></pre></td></tr></table></figure>
<p>This will output content similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Permute(Shift) torch.Size([224, 224, 3]) to torch.Size([3, 224, 224])</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>学习人生</category>
        <category>Theories</category>
        <category>张量计算</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>AI</tag>
        <tag>DeepLearning</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>变更记录</title>
    <url>/2000/01/01/%E5%8F%98%E6%9B%B4%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h1>2022</h1>
<h2 id="aug-27-2022">Aug 27, 2022</h2>
<ul>
<li>将Blog从旧域名迁移至新域名</li>
</ul>
<h2 id="may-2-2022">May 2, 2022</h2>
<ul>
<li>将Github Page上托管的Blog仓库迁移至主服务器</li>
</ul>
<h2 id="apr-16-2022">Apr 16, 2022</h2>
<ul>
<li>上传Blog仓库到Github，并且托管至Github Page</li>
</ul>
<hr>
<h1>2023</h1>
<h2 id="备考高考">备考高考</h2>
<h2 id="mar-5-2023">Mar 5, 2023</h2>
<ul>
<li>将Blog从旧域名迁移至新域名</li>
</ul>
<hr>
<h1>2024</h1>
<h2 id="进入大学">进入大学</h2>
<ul>
<li>停更</li>
</ul>
<h2 id="高考">高考</h2>
<ul>
<li>停更</li>
</ul>
<hr>
<h1>2025</h1>
<h2 id="aug-12-2022">Aug 12, 2022</h2>
<ul>
<li>开始更新</li>
</ul>
<h2 id="aug-12-2022">Aug 12, 2022</h2>
<ul>
<li>将Blog从旧域名迁移至新域名</li>
</ul>
]]></content>
      <tags>
        <tag>SysRecord</tag>
      </tags>
  </entry>
  <entry>
    <title>学习人生之C++进阶</title>
    <url>/2024/01/01/%E5%AD%A6%E4%B9%A0%E4%BA%BA%E7%94%9F%E4%B9%8BC++%E8%BF%9B%E9%98%B6/</url>
    <content><![CDATA[<p>注意，不是你卡了，是我还没写任何的内容 :(</p>
<span id="more"></span>]]></content>
      <categories>
        <category>学习人生</category>
        <category>Language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Development</tag>
        <tag>C++ Lang</tag>
      </tags>
  </entry>
  <entry>
    <title>利用JACK , 玩转 Reaper For Linux</title>
    <url>/2022/10/28/%E5%88%A9%E7%94%A8JACK%20,%20%E7%8E%A9%E8%BD%AC%20Reaper%20For%20Linux/</url>
    <content><![CDATA[<p><img src="../images/posts/JACK-Reaper-For-Linux/Cover.png" alt="Cover"></p>
<h1>写在前面</h1>
<p>笔者在这里用的 Linux 发行版是 <code>Arch Linux</code> , 并且使用 <a href="https://www.reaper.fm/">Reaper 官网</a> 所提供的 <code>Linux x86_64</code> 版本。</p>
<p>不同的系统有不同的操作方式,但是大体逻辑是一样的。这篇Post的目的在于成功的利用Jack在Linux上运行daw。并且由于Reaper在linux有提供Build包,所以我们应当首选Reaper来作为我们要使用的Daw，这样会节省很多时间。</p>
<span id="more"></span>
<h1>1.安装，启动并设定<code>JACK2</code></h1>
<blockquote>
<p>参考： <a href="https://anclark.github.io/2020/12/08/Linux_Audio_Production/Make_Your_ArchLinux_An_Audio_Workstation_01/">把Arch Linux打造成音乐工作站</a></p>
</blockquote>
<p><code>JACK</code> 分为两个版本，分别是：</p>
<ul>
<li><code>JACK1</code> 由C编写，这是最初的版本</li>
<li><code>JACK2</code> 由C++重写</li>
</ul>
<p><code>JACK2</code>相比<code>JACK1</code>来说，要方便配置很多，它不需要配置多的用户权限就可以使用，并且可以把<code>PulseAudio</code>,<code>ALSA</code> 桥接到 <code>JACK2</code> 。并且如果你之前在Windows体验过被ASIO4ALL所独占声卡的悲痛，那么<code>JACK1</code>也是如此。<br>
所以我们果断选择<code>JACK2</code>。</p>
<h2 id="1-安装">1.安装</h2>
<h3 id="打开您的terminal-首先安装jack2主程序：">打开您的Terminal，首先安装<code>JACK2</code>主程序：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo pacman -S jack2</span><br></pre></td></tr></table></figure>
<h3 id="根据需求安装您需要桥接到jack2的接口：">根据需求安装您需要桥接到<code>JACK2</code>的接口：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo pacman -S pulseaudio-jack <span class="comment">#这是PulseAudio -&gt; JACK2</span></span><br><span class="line">$ sudo pacman -S zita-ajbridge   <span class="comment">#这是ALSA -&gt; JACK2</span></span><br></pre></td></tr></table></figure>
<h3 id="安装cadence来托管jack2：">安装<code>Cadence</code>来托管<code>JACK2</code>：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo pacman -S cadence</span><br></pre></td></tr></table></figure>
<h3 id="添加您到audio用户组：">添加您到<code>audio</code>用户组：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo usermod --append -G audio username  <span class="comment">#其中username是您的用户名</span></span><br></pre></td></tr></table></figure>
<h3 id="final-step：">Final Step：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ reboot</span><br></pre></td></tr></table></figure>
<h2 id="2-启动并设定jack2">2.启动并设定<code>JACK2</code></h2>
<h3 id="启动-cadence-通常来说-它安装到了您的桌面-只需要搜索就可以启动它">启动 <code>Cadence</code>，通常来说，它安装到了您的桌面，只需要搜索就可以启动它。</h3>
<p><img src="/images/posts/JACK-Reaper-For-Linux/1.png" alt="1.2.1" title="1.2.1"></br></p>
<h3 id="点击-jack-bridges-中的-bridge-type-来设定桥接-这里根据您所安装的接口来选择-我选择了alsa-jack-：">点击 <code>JACK Bridges</code> 中的 <code>Bridge Type</code> 来设定桥接 （这里根据您所安装的接口来选择，我选择了<code>ALSA -&gt; JACK</code>）：</h3>
<p><img src="/images/posts/JACK-Reaper-For-Linux/2.png" alt="1.2.2" title="1.2.2"></br></p>
<h3 id="点击-jack-status-中的-configure-来配置桥接-您设定的桥接是什么接口就选择什么driver-：">点击 <code>JACK Status</code> 中的 <code>Configure</code> 来配置桥接 （您设定的桥接是什么接口就选择什么<code>Driver</code>）：</h3>
<p><img src="/images/posts/JACK-Reaper-For-Linux/3.png" alt="1.2.3" title="1.2.3"></br></p>
<h3 id="点击ok-退回到主界面之后点击start即可启动jack2-您也可以点击auto-start-jack-or-ladish-at-login来登陆自动启动jack2-：">点击<code>OK</code> 退回到主界面之后点击<code>Start</code>即可启动<code>JACK2</code>。（您也可以点击<code>Auto-start JACK or LADISH at login</code>来登陆自动启动<code>JACK2</code>）：</h3>
<p><img src="/images/posts/JACK-Reaper-For-Linux/4.png" alt="1.2.4" title="1.2.4"></br><br>
看到<code>JACK Status</code> 中的 <code>Server Status</code> 是<code>Start</code>就说明 <code>Cadence</code>已经托管了<code>JACK2</code>并且<code>JACK2</code>也桥接了我们的音频设备。（后续您就可以为<code>Cadence</code>添加自启动项，这样一开机音频驱动就会自动桥接到JACK2啦）</p>
<h1>2.安装，启动并配置<code>Reaper</code></h1>
<h2 id="1-安装">1.安装</h2>
<h3 id="首先去-reaper的官网-然后点击download-reaper-来下载reaper：">首先去 <a href="https://www.reaper.fm/">Reaper的官网</a> 然后点击<code>DOWNLOAD REAPER</code> 来下载<code>Reaper</code>：</h3>
<blockquote>
<p>此处根据您的系统架构选择版本（<code>一般选择Linux x86_64</code>）：</p>
</blockquote>
<h3 id="解压-进入主程序目录：">解压,进入主程序目录：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tar -xvf reaper669_linux_x86_64.tar.xz &amp;&amp; <span class="built_in">rm</span> reaper669_linux_x86_64.tar.xz &amp;&amp; <span class="built_in">cd</span> reaper669_linux_x86_64  <span class="comment">#注意，这里669根据版本号来自行更改哦</span></span><br></pre></td></tr></table></figure>
<h3 id="在当前目录执行安装脚本：">在当前目录执行安装脚本：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">chmod</span> +x install-reaper.sh</span><br><span class="line">$ ./install-reaper.sh  </span><br></pre></td></tr></table></figure>
<p>输入<code>I</code>，之后<code>Reaper</code>会被安装到<code>～/opt/REAPER/</code>目录下</p>
<h3 id="软连接到-usr-bin">软连接到<code>/usr/bin</code></h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo <span class="built_in">ln</span> -s /home/username/opt/REAPER/reaper /usr/bin/reaper  <span class="comment">#其中username是您的用户名</span></span><br></pre></td></tr></table></figure>
<h3 id="写desktop-file以便于从桌面启动：">写<code>Desktop File</code>以便于从桌面启动：</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo vim /usr/share/applications/reaper.desktop</span><br></pre></td></tr></table></figure>
<p>按<code>I</code>进入插入模式，输入：</p>
<pre><code class="language-bash">[Desktop Entry]  
Name=Reaper  
Comment=A Complete Digital Audio Production Application For Linux  
Exec=/usr/bin/reaper  
Icon=/home/username/opt/Reaper/reaper.png  #您需要去下载一个reaper的Icon移动到这里，其中username是您的用户名  
Terminal=false  
Type=Application  
Categories=Music;
</code></pre>
<h3 id="现在您搜索reaper应该就有匹配项了-如果没有-您可能需要等一小会儿">现在您搜索<code>Reaper</code>应该就有匹配项了（如果没有，您可能需要等一小会儿）</h3>
<h2 id="2-启动并配置reaper">2.启动并配置<code>Reaper</code></h2>
<h3 id="打开reaper-无论您是从命令行启动-还是桌面启动">打开<code>Reaper</code>，无论您是从命令行启动，还是桌面启动</h3>
<h3 id="等待几秒的许可证要求窗口-然后点击still-evaluating">等待几秒的许可证要求窗口，然后点击<code>Still Evaluating</code></h3>
<p><img src="/images/posts/JACK-Reaper-For-Linux/5.png" alt="2.2.1" title="2.2.1"></br></p>
<blockquote>
<p><code>Reaper</code>是一款轻量强大的DAW，如果您觉得它还不错，并且您已经在Windows使用Reaper许久，却还没有注册license的话，请支持制作商<code>Cockos</code>，买一份<code>Reaper</code>的License吧！</p>
</blockquote>
<h3 id="点击options-preferences-audio-device-选择audio-system-jack-然后apply即可">点击<code>Options-&gt;Preferences-&gt;Audio-&gt;Device</code>，选择<code>Audio system</code>-&gt;<code>JACK</code>，然后<code>Apply</code>即可。</h3>
<p><img src="/images/posts/JACK-Reaper-For-Linux/6.png" alt="2.2.2" title="2.2.3"></br></p>
<h3 id="恭喜您-配置完成了-现在开始reaper已经以jack2为音频驱动-为您工作了">恭喜您，配置完成了！现在开始<code>Reaper</code>已经以<code>JACK2</code>为音频驱动，为您工作了！</h3>
<h1>Wait… Somethings is Wrong…</h1>
<p>你有没有发现，我在封面展示的那张图中，显示了中文，但是当您兴冲冲的开始自己的Linux音乐之路的时候，发现您的输入中文或者是其它字符时却出现了方块！</p>
<h2 id="这个问题的解决方法来自cockos-reaper-中文群的大佬tee">这个问题的解决方法来自<code>Cockos REAPER 中文群</code>的大佬<a href="https://lado.me/">Tee</a></h2>
<blockquote>
<p>原帖地址是这里：<a href="https://zhuanlan.zhihu.com/p/364000143">如何在Linux REAPER里显示中文</a></p>
</blockquote>
<pre><code class="language-bash">sudo vim /etc/fonts/fonts.conf
</code></pre>
<p>按下<code>I</code>，插入以下内容：</p>
<pre><code class="language-bash">    &lt;!-- REAPER fonts patch --&gt;
    &lt;match target=&quot;pattern&quot;&gt;
      &lt;test name=&quot;prgname&quot;&gt;
       &lt;string&gt;reaper&lt;/string&gt;
    &lt;/test&gt;
      &lt;edit name=&quot;family&quot; mode=&quot;assign&quot;&gt;
       &lt;string&gt;Noto Sans CJK SC&lt;/string&gt;
      &lt;/edit&gt;
    &lt;/match&gt;  
</code></pre>
<p>注意插入的位置，位于 <strong>fontconfig标签之间。</strong> 这里涉及系统文件，请务必小心。</p>
<h3 id="现在输入中文-它的显示就会正常了">现在输入中文，它的显示就会正常了！</h3>
<p align="right">在最后的最后，感恩</p>
<p align="right">Romi Brooks♥</p>]]></content>
      <categories>
        <category>Linux</category>
        <category>Music</category>
      </categories>
      <tags>
        <tag>Music</tag>
        <tag>Reaper</tag>
      </tags>
  </entry>
  <entry>
    <title>学习人生之数据分析</title>
    <url>/2025/11/04/%E5%AD%A6%E4%B9%A0%E4%BA%BA%E7%94%9F%E4%B9%8BDataAnalysis/</url>
    <content><![CDATA[<p>注意，不是你卡了，是我还没写任何的内容 :(</p>
<span id="more"></span>]]></content>
      <categories>
        <category>学习人生</category>
        <category>Theories</category>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Python</tag>
        <tag>DataAnalysis</tag>
      </tags>
  </entry>
  <entry>
    <title>学习人生之初见Vue3</title>
    <url>/2025/11/03/%E5%AD%A6%E4%B9%A0%E4%BA%BA%E7%94%9F%E4%B9%8B%E5%88%9D%E8%A7%81Vue3/</url>
    <content><![CDATA[<h1>什么是Vue?</h1>
<p>Vue 是一套用于构建用户界面的渐进式 JavaScript 框架，其特点：易学易用，性能出色，适用场景丰富的 Web 前端框架。</p>
<p>Vue基于 Nodejs，目前 Vue 的最新稳定版本是 Vue 3，引入了 Composition API、TypeScript 支持等新特性。</p>
<p>本文基于Vue3，记录从零开始学习Vue3框架的历程。</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习人生</category>
        <category>WebDev</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Development</tag>
        <tag>Vue3</tag>
        <tag>Javascript</tag>
        <tag>Typescript</tag>
        <tag>frontend</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx二级域名配置</title>
    <url>/2023/02/25/Nginx%E4%BA%8C%E7%BA%A7%E5%9F%9F%E5%90%8D%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<div style="text-align: center;">  
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=2020918656&auto=0&height=66"></iframe>  
<p><s>YOASOBI 在 2月15号出新专辑了捏</s></p>
</div>  
<blockquote>
<p>在去年Digital Ocean把我的账号突然封禁之后,我甚至没有办法迁移我的数据,所有配置文件只能重写<br>
这里是为了留一份Nginx中二级域名.conf配置文件的写法,依据我的习惯,仅供参考</p>
</blockquote>
<p>Nginx现在的版本应该会在<code>/etc/nginx/</code>目录存在<code>nginx.conf</code><br>
这里大概是这样</p>
<span id="more"></span>  
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attribute">user</span>  nginx;</span><br><span class="line"><span class="attribute">worker_processes</span>  auto;</span><br><span class="line"></span><br><span class="line"><span class="attribute">error_log</span>  /var/log/nginx/<span class="literal">error</span>.log <span class="literal">notice</span>;</span><br><span class="line"><span class="attribute">pid</span>        /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">events</span> &#123;</span><br><span class="line">    <span class="attribute">worker_connections</span>  <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    <span class="attribute">include</span>       /etc/nginx/mime.types;</span><br><span class="line">    <span class="attribute">default_type</span>  application/octet-stream;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">log_format</span>  main  <span class="string">&#x27;<span class="variable">$remote_addr</span> - <span class="variable">$remote_user</span> [<span class="variable">$time_local</span>] &quot;<span class="variable">$request</span>&quot; &#x27;</span></span><br><span class="line">                      <span class="string">&#x27;<span class="variable">$status</span> <span class="variable">$body_bytes_sent</span> &quot;<span class="variable">$http_referer</span>&quot; &#x27;</span></span><br><span class="line">                      <span class="string">&#x27;&quot;<span class="variable">$http_user_agent</span>&quot; &quot;<span class="variable">$http_x_forwarded_for</span>&quot;&#x27;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">access_log</span>  /var/log/nginx/access.log  main;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">sendfile</span>        <span class="literal">on</span>;</span><br><span class="line">    <span class="comment">#tcp_nopush     on;</span></span><br><span class="line"></span><br><span class="line">    <span class="attribute">keepalive_timeout</span>  <span class="number">65</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#gzip  on;</span></span><br><span class="line"></span><br><span class="line">    <span class="attribute">include</span> /etc/nginx/conf.d/<span class="regexp">*.conf</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可见,<code>nginx.conf</code>引用了<code>/etc/nginx/conf.d/</code>下的所有.conf文件<br>
这里我习惯把我的个人网站配置文件放在这</p>
<h1>Start</h1>
<p>现在假设我想要创建一个名为<code>blog.romichan.me</code>的网站,用来存放我的博客<br>
首先在你域名的托管商配置<code>DNS解析</code>,可以是A解析,也可以是AAAA解析,需要绑定到服务器上<br>
接着在<code>/etc/nginx/conf.d/</code>下新建一个名为<code>blog.ssl.conf</code>的文件,他一般是这样的:</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span>  <span class="number">443</span> ssl;    <span class="comment"># 自从新版本nginx开始,启用ssl隧道得这么写</span></span><br><span class="line">    <span class="attribute">ssl_certificate</span> /etc/letsencrypt/live/blog.romichan.me/fullchain.pem;   <span class="comment"># 证书文件</span></span><br><span class="line">    <span class="attribute">ssl_certificate_key</span> /etc/letsencrypt/live/blog.romichan.me/privkey.pem; <span class="comment"># 证书密钥文件</span></span><br><span class="line">    <span class="attribute">server_name</span>  blog.romichan.me;  <span class="comment">#这里就是要解析到的域名,也就是你在域名托管商的DNS解析里所设置的</span></span><br><span class="line">        <span class="section">location</span> / &#123;</span><br><span class="line">            <span class="attribute">root</span>         /usr/share/webpage/blog; <span class="comment">#这里是博客的根目录.据你的需求,我只需要index.html所以下面的index只写了这些</span></span><br><span class="line">            <span class="attribute">index</span>        index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而后我们在新建一个文件,它的名字是<code>blog.conf</code>,写入这些:</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span>  <span class="number">80</span>; <span class="comment"># 不作解释,8080也行</span></span><br><span class="line">    <span class="attribute">server_name</span>  blog.romichan.me; <span class="comment"># 解析到的网站</span></span><br><span class="line">    <span class="attribute">rewrite</span><span class="regexp"> ^(.*)$</span>  https://<span class="variable">$host</span><span class="variable">$1</span> <span class="literal">permanent</span>; <span class="comment"># 重点,这里是把网页强行重定向到https,这样我们上面的证书才可以起到效果!</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总体就这么多,如果你需要通过二级域名来转发你服务器某个端口的数据 (应该是叫<code>反向代理</code>) ,也是这么写,不过需要修改<code>blog.ssl.conf</code>的一些地方:</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line">server&#123;</span><br><span class="line">    <span class="attribute">listen</span>  <span class="number">443</span> ssl;</span><br><span class="line">    <span class="attribute">ssl_certificate</span> /etc/letsencrypt/live/blog.romichan.me/fullchain.pem;</span><br><span class="line">    <span class="attribute">ssl_certificate_key</span> /etc/letsencrypt/live/blog.romichan.me/privkey.pem;</span><br><span class="line">    <span class="attribute">server_name</span>  blog.romichan.me;</span><br><span class="line"></span><br><span class="line">    <span class="section">location</span> / &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span>  http://127.0.0.1:8080; <span class="comment"># 这里就是转发的地址了</span></span><br><span class="line">        <span class="attribute">proxy_set_header</span> Host <span class="variable">$proxy_host</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>That’s all</h1>
<p>这就是一个随笔</p>
<p>我当时在搜集这些资料的时候被很多的名字都吓到了,但其实这是一个很简单的东西,完全是一个模板化的东西,只需要记下来,以后应该都是适用的</p>
<p>大概就这么多,如果你看到了,希望对你有帮助.</p>
<p align="right">在最后的最后，感恩</p>
<p align="right">Romi Brooks♥</p>]]></content>
      <categories>
        <category>WebDev</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>WebDev</tag>
        <tag>Website</tag>
      </tags>
  </entry>
</search>
